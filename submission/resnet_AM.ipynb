{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\phili/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models   \n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from numpy import asarray, percentile, tile\n",
    "import json\n",
    "import requests\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')\n",
    "\n",
    "resnet50 = models.resnet50(pretrained = True)\n",
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n",
    "\n",
    "resnet50.eval().to(device)\n",
    "\n",
    "alexnet = models.alexnet(weights='IMAGENET1K_V1')\n",
    "alexnet.eval().to(device)\n",
    "\n",
    "# Normalize images for final displaying\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "denormalize = transforms.Normalize(mean = [-0.485/0.229, -0.456/0.224, -0.406/0.225], std = [1/0.229, 1/0.224, 1/0.225] )\n",
    "def image_converter(im):\n",
    "    im_copy = im.cpu()\n",
    "    \n",
    "    im_copy = denormalize(im_copy.clone().detach()).numpy()\n",
    "    im_copy = im_copy.transpose(1,2,0)\n",
    "    im_copy = im_copy.clip(0, 1) \n",
    "    return im_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute jitter loss\n",
    "def jitter(model, img, jitter_t, layer_activation, layer_name, unit):\n",
    "    sum = 0\n",
    "    for i in range(10):\n",
    "        temp_img = img\n",
    "        tao = random.randint(0, jitter_t)\n",
    "        temp_img = torch.add(temp_img, tao)\n",
    "        model(temp_img)\n",
    "        layer_out = layer_activation[layer_name]\n",
    "        sum = torch.add(sum, layer_out[0][unit])\n",
    "    jitter_loss = sum / 10\n",
    "    jitter_loss.requires_grad_(True)\n",
    "    jitter_loss.retain_grad() \n",
    "    jitter_loss.backward(retain_graph=True)\n",
    "    jitter_grad = jitter_loss.grad.detach()\n",
    "    return jitter_loss.detach(), jitter_grad\n",
    "# Compute TV loss\n",
    "def tv(img, img_grad):\n",
    "    bs_img, c_img, h_img, w_img = img.size()\n",
    "    w = torch.sum(torch.pow(img[:,:,:,:-1] - img[:,:,:,1:], 2))\n",
    "    h = torch.sum(torch.pow(img[:,:,:-1,:] - img[:,:,1:,:], 2))\n",
    "    tv_loss = (1/(torch.norm(img_grad) * h_img * w_img) * (h + w))\n",
    "    tv_loss.requires_grad_(True)\n",
    "    tv_loss.retain_grad() \n",
    "    tv_loss.backward(retain_graph=True)\n",
    "    tv_grad = tv_loss.grad.detach()\n",
    "    return tv_loss.detach(), tv_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximize activation of a single input image with regularization\n",
    "def act_max(model, \n",
    "    inp_img, \n",
    "    layer_activation, \n",
    "    layer_name, \n",
    "    unit, \n",
    "    steps=100, \n",
    "    alpha=torch.tensor(1),\n",
    "    TV = False,\n",
    "    Jitter = False,\n",
    "    Regular = False,\n",
    "    jitter_t = 20,\n",
    "    jitter_alpha = 0.05,\n",
    "    tv_alpha = 0.05,\n",
    "    show_img = False\n",
    "    ):\n",
    "\n",
    "    best_activation = -float('inf')\n",
    "    min_loss = float('inf')\n",
    "    best_img = inp_img\n",
    "    for k in range(steps):\n",
    "        inp_img.requires_grad_(True)\n",
    "        inp_img.retain_grad() \n",
    "        inp_img = inp_img.to(device)\n",
    "        old_norm = torch.norm(inp_img)\n",
    "        # Propagate image\n",
    "        model(inp_img)\n",
    "        layer_out = layer_activation[layer_name]\n",
    "        # Compute gradients\n",
    "        layer_out[0][unit].backward(retain_graph=True)\n",
    "        img_grad = inp_img.grad\n",
    "            \n",
    "        # Gradient Step\n",
    "        inp_img = torch.add(inp_img, torch.mul(img_grad.detach(), alpha))\n",
    "\n",
    "        act_loss = layer_out[0][unit]\n",
    "        #Jitter\n",
    "        jitter_loss = torch.tensor(0)\n",
    "        jitter_grad = 0\n",
    "        if Jitter and k % 10 == 0:\n",
    "            jitter_loss, jitter_grad = jitter(model, inp_img, jitter_t, layer_activation, layer_name, unit)\n",
    "        if TV:\n",
    "            tv_loss, tv_grad = tv(inp_img, img_grad)\n",
    "\n",
    "        # Keep highest activation\n",
    "        loss = -1 * act_loss\n",
    "        if Jitter:\n",
    "            loss -= jitter_alpha * jitter_loss\n",
    "        if TV:\n",
    "            loss += tv_alpha * tv_loss\n",
    "        if Regular:\n",
    "            if Jitter:\n",
    "              inp_img = torch.add(inp_img, torch.mul(jitter_grad, alpha*jitter_alpha))\n",
    "            if TV:\n",
    "              inp_img = torch.add(inp_img, torch.mul(tv_grad, -alpha*tv_alpha))\n",
    "        \n",
    "        new_norm = torch.norm(inp_img)\n",
    "        inp_img = torch.mul(inp_img, old_norm/new_norm)\n",
    "        if loss < min_loss:\n",
    "            if not Jitter or k % 10 != 10:\n",
    "                jitter_loss, jitter_grad = jitter(model, inp_img, jitter_t, layer_activation, layer_name, unit)\n",
    "            if not TV:\n",
    "                tv_loss, tv_grad = tv(inp_img, img_grad)\n",
    "            best_activation = act_loss, jitter_loss, tv_loss, loss\n",
    "            min_loss = loss\n",
    "            best_img = inp_img\n",
    "\n",
    "        if show_img and k == steps-1:\n",
    "            final_image = image_converter(inp_img.squeeze(0))\n",
    "            plt.imshow(final_image)\n",
    "            plt.show()        \n",
    "            print('step: ', k, 'activation: ', layer_out[0][unit])\n",
    "        \n",
    "    return (best_activation, best_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for getting gradients\n",
    "def layer_hook(act_dict, layer_name):\n",
    "    def hook(module, input, output):\n",
    "        act_dict[layer_name] = output\n",
    "    return hook\n",
    "# Random image initialization\n",
    "def reset_img():\n",
    "  inp = torch.rand((1, 3, 227, 227))\n",
    "  inp.requires_grad_(True)\n",
    "  return inp.to(device)\n",
    "# Convert (1, 3, 227, 227) Torch tensor into 227*227 element numpy array, averaging across RGB channels\n",
    "def np_data(img):\n",
    "    img = denormalize(img.squeeze().detach().cpu())\n",
    "    img = torch.mean(img, 0)\n",
    "    img = torch.flatten(img)\n",
    "    img = img.numpy()\n",
    "    return img\n",
    "# Displaying data in a histogram to visualize activation distribution\n",
    "def get_hist(arr, title):\n",
    "    hist, bin = np.histogram(arr)\n",
    "    plt.hist(arr, bins=bin)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = 130\n",
    "steps = 200\n",
    "alpha = torch.tensor(1.5)\n",
    "# Main method: activation maximization over 'trials' times, computing all 4 kinds of \n",
    "# losses (activation score, jitter loss, TV loss, and total (weighted) loss)\n",
    "def experiment(model, TV, Jitter, jitter_t=0, jitter_alpha=0, tv_alpha=0, trials=10):\n",
    "    # In order: activation, jitter, tv, and total losses\n",
    "    losses = [], [], [], []\n",
    "    for t in range(trials):\n",
    "        # starting image\n",
    "        orig_img = reset_img()\n",
    "        inp = orig_img\n",
    "        # outputs of image through both neural nets\n",
    "        results = model(inp)\n",
    "        value = results.detach().cpu().numpy()\n",
    "        # max outputs of image through both neural nets\n",
    "        k = max(value[0])\n",
    "        act_dict = {}\n",
    "        layer_name = 'classifier_final'\n",
    "        list(model.children())[-1].register_forward_hook(layer_hook(act_dict, layer_name))\n",
    "        \n",
    "        activation, output = act_max(model=model,\n",
    "                    inp_img=inp,\n",
    "                    layer_activation=act_dict,\n",
    "                    layer_name=layer_name,\n",
    "                    unit=unit,\n",
    "                    steps=steps,\n",
    "                    alpha=alpha,\n",
    "                    TV=TV,\n",
    "                    Jitter=Jitter,\n",
    "                    Regular=True,\n",
    "                    jitter_t=jitter_t,\n",
    "                    jitter_alpha=jitter_alpha,\n",
    "                    tv_alpha=tv_alpha,\n",
    "                    show_img=False,\n",
    "                    )\n",
    "        for i in range(4):\n",
    "            if isinstance(activation[i], int):\n",
    "                print(i)\n",
    "            losses[i].append(activation[i].detach().cpu().numpy().item())\n",
    "        out = np_data(output)\n",
    "        torch.cuda.empty_cache()\n",
    "    names = [\"Activations:\", \"Jitter losses:\", \"TV losses:\", \"Total losses:\"], [\"Average activation:\", \"Average jitter loss:\", \"Average TV loss:\", \"Average total loss:\"]\n",
    "    # for i in range(4):\n",
    "    #     # print(names[0][i], str(losses[i]))\n",
    "    #     # print(names[1][i], sum(losses[i])/trials)\n",
    "    data = [losses[0][0] ,losses[1][0],losses[2][0],losses[3][0]]\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example results from an experiment\n",
    "resnet50, TV=False, Jitter=True, jitter_t=10, jitter_alpha=0.1, tv_alpha=0, trials=10  \n",
    " - Activations: [163.57022094726562, 171.38491821289062, 177.36441040039062, 184.45797729492188, 174.5690460205078, 162.1881103515625, 173.3818359375, 167.202392578125, 176.84043884277344, 178.08633422851562]\n",
    " - Average activation: 172.90456848144532\n",
    " - Jitter losses: [22.103370666503906, 34.036346435546875, 8.708541870117188, 19.32915496826172, 38.63886260986328, 27.92671775817871, 14.243896484375, 23.848962783813477, 12.266249656677246, 39.02633285522461]\n",
    " - Average jitter loss: 24.0128436088562\n",
    " - TV losses: [0.02318560890853405, 0.027121976017951965, 0.028241293504834175, 0.04551585391163826, 0.025176187977194786, 0.026284033432602882, 0.026428097859025, 0.02534600719809532, 0.028803151100873947, 0.029583383351564407]\n",
    " - Average TV loss: 0.02856855932623148\n",
    " - Total losses: [-167.22222900390625, -171.38491821289062, -180.71128845214844, -184.45797729492188, -174.5690460205078, -164.54434204101562, -178.4657440185547, -170.62872314453125, -180.0749053955078, -181.38262939453125]\n",
    " - Average total loss: -175.34418029785155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing for alexnet\n",
      "209.92312622070312,-0.07909908145666122,0.01727479323744774,-963.8392944335938\n",
      "254.4646759033203,2.6975669860839844,0.022933119907975197,-1035.287109375\n",
      "507.0416564941406,4.948776721954346,0.12290405482053757,-2624.91796875\n",
      "836.6007080078125,14.898063659667969,0.32265886664390564,-4679.0009765625\n",
      "194.5734405517578,-0.2849178910255432,0.016341088339686394,-1260.2451171875\n",
      "218.1263427734375,-1.0432029962539673,0.021373311057686806,-1501.546875\n",
      "397.50030517578125,6.869326114654541,0.10230516642332077,-3092.896484375\n",
      "726.5560913085938,5.282922267913818,0.335868239402771,-6080.73828125\n",
      "115.45935821533203,-1.5717108249664307,0.008543641306459904,-700.4129028320312\n",
      "122.19690704345703,-1.8676187992095947,0.010542846284806728,-1001.8298950195312\n",
      "378.70477294921875,1.9961864948272705,0.08805258572101593,-3241.064208984375\n",
      "624.8535766601562,5.756803035736084,0.3534456491470337,-6178.48095703125\n",
      "67.59841918945312,-1.7852413654327393,0.005208959802985191,-623.195556640625\n",
      "55.31923294067383,-1.640119194984436,0.0044278367422521114,-447.0452880859375\n",
      "231.95738220214844,-1.413573980331421,0.044172681868076324,-2828.801025390625\n",
      "588.8843994140625,4.019261360168457,0.3405837416648865,-8188.27734375\n",
      "261.903564453125,2.4195659160614014,0.02230568788945675,-1193.087646484375\n",
      "219.3976593017578,0.0980185940861702,0.021529627963900566,-1008.7683715820312\n",
      "479.33380126953125,-1.4494061470031738,0.11765759438276291,-2363.893310546875\n",
      "800.1099243164062,19.204179763793945,0.31682732701301575,-4561.73828125\n",
      "205.28309631347656,0.4444088935852051,0.01575414277613163,-1332.8941650390625\n",
      "198.1738739013672,-0.9300317764282227,0.02030455879867077,-1383.904052734375\n",
      "401.0987548828125,6.173476696014404,0.09310582280158997,-3029.012451171875\n",
      "757.0897216796875,8.179606437683105,0.3275178372859955,-6143.8994140625\n",
      "145.5930938720703,-1.4921075105667114,0.01179162785410881,-1067.7662353515625\n",
      "147.6181182861328,-1.307251214981079,0.012304617092013359,-1246.573974609375\n",
      "392.77667236328125,1.3590530157089233,0.08508733659982681,-3549.83251953125\n",
      "727.3339233398438,-0.29761752486228943,0.3377377986907959,-7008.55419921875\n",
      "83.50811767578125,-1.8580998182296753,0.005544420797377825,-986.7445068359375\n",
      "45.93986129760742,-1.9062178134918213,0.003956373315304518,-440.8667297363281\n",
      "264.5231018066406,-1.762682557106018,0.05059579387307167,-3355.527099609375\n",
      "603.7984008789062,1.993652582168579,0.3489563763141632,-8272.0009765625\n",
      "235.89776611328125,-0.007595503237098455,0.02020152471959591,-1097.6029052734375\n",
      "260.06146240234375,0.3444398045539856,0.024409862235188484,-1061.714599609375\n",
      "491.8892822265625,3.3173177242279053,0.11607116460800171,-2497.27099609375\n",
      "810.7018432617188,12.730460166931152,0.3273279070854187,-4476.36474609375\n",
      "171.62696838378906,-1.7411707639694214,0.014324380084872246,-851.3148193359375\n",
      "254.05555725097656,-0.5184108018875122,0.021962996572256088,-1436.6396484375\n",
      "466.8086242675781,0.4592495560646057,0.10300921648740768,-3345.117919921875\n",
      "732.7576293945312,10.140922546386719,0.33985233306884766,-5861.140625\n",
      "83.04259490966797,-1.7868908643722534,0.006431334652006626,-591.9376831054688\n",
      "130.16705322265625,-0.5182284712791443,0.012074711732566357,-1017.2125854492188\n",
      "447.15606689453125,1.9738835096359253,0.09958107769489288,-3770.599609375\n",
      "667.8491821289062,2.374570608139038,0.34050509333610535,-6528.37744140625\n",
      "45.05019760131836,-1.7683671712875366,0.004611905664205551,-490.9013671875\n",
      "54.83034896850586,-1.5573352575302124,0.004855062812566757,-473.9478759765625\n",
      "218.55430603027344,-1.7186224460601807,0.03763789311051369,-2502.443603515625\n",
      "633.3709106445312,2.206900119781494,0.334749698638916,-9610.6611328125\n",
      "241.37696838378906,-0.406006783246994,0.021827561780810356,-1014.9760131835938\n",
      "283.2734069824219,-0.4924468994140625,0.02877211757004261,-1175.799072265625\n",
      "550.1732788085938,14.334769248962402,0.12032850831747055,-2964.95947265625\n",
      "829.7367553710938,2.367108106613159,0.31519684195518494,-4332.625\n",
      "174.40577697753906,-0.04002947732806206,0.015641171485185623,-1110.08154296875\n",
      "200.23513793945312,-0.46071091294288635,0.01751837506890297,-1213.35693359375\n",
      "403.9849853515625,-0.40921875834465027,0.10226356983184814,-2852.228759765625\n",
      "689.0780639648438,3.487752676010132,0.34962791204452515,-5444.67041015625\n",
      "180.29273986816406,-1.4946209192276,0.013334923423826694,-1399.0638427734375\n",
      "185.25672912597656,-1.368056297302246,0.016983546316623688,-1377.75390625\n",
      "361.18157958984375,2.7388572692871094,0.07424906641244888,-2935.412841796875\n",
      "721.66015625,4.835774898529053,0.3395661413669586,-7439.4619140625\n",
      "54.81692123413086,-1.6605348587036133,0.004591257311403751,-397.4747314453125\n",
      "75.73770904541016,-1.753533959388733,0.007018542382866144,-574.6978759765625\n",
      "217.9609375,-1.6565223932266235,0.03876993805170059,-2311.36181640625\n",
      "565.0009155273438,6.7722907066345215,0.35275015234947205,-7652.22900390625\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# Experiment we ran for AlexNet\n",
    "with open('alexnet_reg.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    jitter_t_values = [5, 8, 10, 15]\n",
    "    jitter_alpha_values = [0.005,0.01,0.05,0.1]\n",
    "    print(\"printing for alexnet\")\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for k in range(4):\n",
    "                data = experiment(alexnet, TV=True, Jitter=True, jitter_t=10, jitter_alpha=jitter_t_values[j], tv_alpha=jitter_alpha_values[k], trials=1)\n",
    "                writer.writerow(data)\n",
    "                print(str(data[0])+\",\"+str(data[1])+\",\"+str(data[2])+\",\"+str(data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing for resnet50\n",
      "printing for resnet50\n",
      "52.32490158081055,-6.062312602996826,0.02914789691567421,-121.46125030517578\n",
      "60.06840515136719,-4.39596700668335,0.03160114958882332,-155.46815490722656\n",
      "75.02816772460938,-1.404200792312622,0.2588936388492584,-159.3423614501953\n",
      "67.95729064941406,-0.014199132099747658,0.6074260473251343,-212.98597717285156\n",
      "39.4609375,-4.094005107879639,0.015002685599029064,-177.2740478515625\n",
      "62.68140411376953,-5.944693565368652,0.021997181698679924,-271.08978271484375\n",
      "59.43431854248047,-3.8177971839904785,0.2237296849489212,-233.4839630126953\n",
      "58.95433044433594,0.6091572642326355,0.6811813712120056,-269.3022155761719\n",
      "37.40721130371094,-5.779452323913574,0.01546125765889883,-219.0736846923828\n",
      "58.025535583496094,-4.8475260734558105,0.01859930530190468,-289.9904479980469\n",
      "60.6517219543457,-5.749935150146484,0.15406280755996704,-277.5993347167969\n",
      "56.377899169921875,-0.10409469902515411,0.6986702084541321,-335.70648193359375\n",
      "41.665061950683594,-4.39353084564209,0.015594224445521832,-239.9845733642578\n",
      "42.995147705078125,-6.637393474578857,0.017676139250397682,-329.9964599609375\n",
      "59.205989837646484,-6.271321773529053,0.09111323952674866,-269.08563232421875\n",
      "50.9682731628418,-4.72413969039917,0.3838948905467987,-240.74423217773438\n",
      "53.44272232055664,-5.715313911437988,0.024187393486499786,-144.43136596679688\n",
      "57.80154037475586,-4.120604515075684,0.01838904246687889,-145.44027709960938\n",
      "68.39964294433594,-2.846642255783081,0.14825263619422913,-185.54718017578125\n",
      "68.529541015625,-0.5555073022842407,0.9278050065040588,-221.39784240722656\n",
      "51.38082504272461,-6.978729248046875,0.018378710374236107,-182.0908660888672\n",
      "41.702091217041016,-4.726039886474609,0.018747825175523758,-149.0676727294922\n",
      "58.67867660522461,-5.976142406463623,0.1650306135416031,-251.0575714111328\n",
      "68.89037322998047,-3.637685537338257,0.4427204132080078,-287.6865234375\n",
      "36.64715576171875,-4.268152713775635,0.015095255337655544,-187.11361694335938\n",
      "31.97980308532715,-3.8243134021759033,0.012466003187000751,-165.1483917236328\n",
      "61.44347381591797,-6.601609706878662,0.19045257568359375,-242.11752319335938\n",
      "51.547367095947266,-7.8686418533325195,0.3821451961994171,-204.25645446777344\n",
      "27.80149269104004,-5.746732711791992,0.00996642280369997,-182.836181640625\n",
      "36.04003143310547,-8.641515731811523,0.016008760780096054,-284.596923828125\n",
      "58.291954040527344,-4.113831996917725,0.08331814408302307,-456.7729797363281\n",
      "58.46479797363281,-5.094549179077148,0.7127395272254944,-407.2218017578125\n",
      "41.66230773925781,-6.144616603851318,0.01921338215470314,-94.53164672851562\n",
      "60.53649139404297,-6.117123603820801,0.027785472571849823,-160.72348022460938\n",
      "76.11617279052734,-3.1674821376800537,0.18775154650211334,-155.77931213378906\n",
      "69.08280181884766,-1.316746711730957,0.6433572173118591,-229.52755737304688\n",
      "46.07673263549805,-5.324163913726807,0.01263065729290247,-176.50782775878906\n",
      "41.30845260620117,-2.7959847450256348,0.01966094970703125,-139.19390869140625\n",
      "50.56317901611328,-4.881674766540527,0.2683029770851135,-257.512939453125\n",
      "52.52229309082031,-4.777132034301758,0.5924984812736511,-302.939208984375\n",
      "47.98983383178711,-7.267037391662598,0.016744215041399002,-206.76126098632812\n",
      "56.175559997558594,-5.4360480308532715,0.018957505002617836,-252.17916870117188\n",
      "53.0718994140625,-6.547181129455566,0.06671959906816483,-367.8297424316406\n",
      "55.14697265625,-4.7333197593688965,0.5756078362464905,-223.84259033203125\n",
      "36.342018127441406,-8.300712585449219,0.015405425801873207,-283.76446533203125\n",
      "43.286128997802734,-1.884644865989685,0.01414635218679905,-239.25558471679688\n",
      "65.22946166992188,-5.119882106781006,0.13458387553691864,-486.5959777832031\n",
      "48.01604461669922,-4.719947338104248,0.44526728987693787,-215.6640625\n",
      "43.54399108886719,-7.823979377746582,0.022871768102049828,-133.0344696044922\n",
      "49.84923553466797,-3.8873653411865234,0.02722814679145813,-142.1732635498047\n",
      "77.77191162109375,-2.1096293926239014,0.1900668889284134,-219.5930938720703\n",
      "64.28197479248047,-5.251763343811035,0.584861695766449,-199.74220275878906\n",
      "45.260623931884766,-2.6940858364105225,0.013083502650260925,-172.70094299316406\n",
      "45.334781646728516,-5.749679088592529,0.021831626072525978,-157.12452697753906\n",
      "60.84645080566406,-3.155439615249634,0.19516247510910034,-220.8179931640625\n",
      "55.89923858642578,-0.4263920485973358,0.5806581377983093,-269.9851989746094\n",
      "44.481666564941406,-4.128115177154541,0.01365939062088728,-192.82882690429688\n",
      "40.51871871948242,-5.055701732635498,0.016053004190325737,-165.42286682128906\n",
      "57.51944351196289,-6.66390323638916,0.04686495289206505,-333.3020324707031\n",
      "58.36713790893555,-3.0984902381896973,0.5928255915641785,-213.41661071777344\n",
      "36.315948486328125,-4.497439861297607,0.01689315401017666,-352.8026123046875\n",
      "29.05193519592285,-4.914800643920898,0.0112710976973176,-277.56768798828125\n",
      "52.338478088378906,-8.009965896606445,0.15597134828567505,-436.06805419921875\n",
      "43.14128112792969,-5.339395046234131,0.6852613091468811,-258.0751953125\n"
     ]
    }
   ],
   "source": [
    "# Experiment we ran for ResNet\n",
    "print(\"printing for resnet50\")\n",
    "with open('resnet50_reg.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    jitter_t_values = [5, 8, 10, 15]\n",
    "    jitter_alpha_values = [0.005,0.01,0.05,0.1]\n",
    "    print(\"printing for resnet50\")\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for k in range(4):\n",
    "                data = experiment(resnet50, TV=True, Jitter=True, jitter_t=10, jitter_alpha=jitter_t_values[j], tv_alpha=jitter_alpha_values[k], trials=1)\n",
    "                writer.writerow(data)\n",
    "                print(str(data[0])+\",\"+str(data[1])+\",\"+str(data[2])+\",\"+str(data[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
